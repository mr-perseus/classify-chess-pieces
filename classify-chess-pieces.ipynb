{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30ff4bd7296d39ba"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:20:37.743758200Z",
     "start_time": "2023-09-08T13:20:35.849244Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test for CUDA\n",
    "Make sure to install the correct CUDA version and packages, see: https://pytorch.org/get-started/locally/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0810cce0901bf62"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {device}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:21:44.787745900Z",
     "start_time": "2023-09-08T13:21:44.784430900Z"
    }
   },
   "id": "c7358d6e495f938a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "beef2e82b83e0f6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create the transformer\n",
    "We resize the image from 640x640 to 32x32, convert them to a tensor, and normalize them with the default values from the ImageNet dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2f742282fbbb73"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 for AlexNet\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:21:45.221524500Z",
     "start_time": "2023-09-08T13:21:45.212984500Z"
    }
   },
   "id": "d35843cbc3d25ab7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the dataset and dataloader"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc2b5ccbbf030131"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "images_path = 'data/images'\n",
    "dataset = datasets.ImageFolder(root=images_path, transform=data_transforms)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:21:45.480471Z",
     "start_time": "2023-09-08T13:21:45.469952300Z"
    }
   },
   "id": "ac7dcfe40c051c1e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Show some images and labels."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "256125cee991731f"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Black bishop', 'Black king', 'Black knight', 'Black pawn', 'Black queen', 'Black rook', 'White bishop', 'White king', 'White knight', 'White pawn', 'White queen', 'White rook']\n",
      "12\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(dataset.classes)\n",
    "print(len(dataset.classes))\n",
    "print(len(dataset))\n",
    "\n",
    "# for images, labels in dataloader:\n",
    "#     print(images)\n",
    "#     print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:21:45.735191800Z",
     "start_time": "2023-09-08T13:21:45.720086100Z"
    }
   },
   "id": "e2be897fa1238bd8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split Train and Test\n",
    "We split test 0.2, to train 0.8."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46a2363a2f424b4a"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "train_size = int(ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:21:45.988310100Z",
     "start_time": "2023-09-08T13:21:45.977471900Z"
    }
   },
   "id": "bda40924cf9e9597"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create the dataloaders\n",
    "We set a feasible batch size (Amount of images is 300).\n",
    "It is important to create the dataloaders after the split!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1de90243757ecf2"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:21:46.231537700Z",
     "start_time": "2023-09-08T13:21:46.229035500Z"
    }
   },
   "id": "90e7f01d8eb918dd"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Train set is 240 images, test set is 60 images. Batch size to 20\n",
    "batch_size = 20\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:21:46.492509100Z",
     "start_time": "2023-09-08T13:21:46.490913100Z"
    }
   },
   "id": "d3557e340032796a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Do the training with AlexNet"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ae2adeec482c0b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Init it"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80f04801b2a94864"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Define the network: AlexNet, num_classes is the number of classes in the dataset (from the dataset directly)\n",
    "net = AlexNet(num_classes=len(dataset.classes))\n",
    "net.to(device)\n",
    "\n",
    "# specify loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer and momentum\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "### DO THE TRAINING!!\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T13:36:16.817143900Z",
     "start_time": "2023-09-08T13:36:16.552202500Z"
    }
   },
   "id": "9d9d1f243a211f48"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aca6c94f624dc4e4"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1ad548e0c727ff5c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
